---
title: "Mapping Overlaps in Benchmarks through Perplexity in the Wild"
collection: publications
category: manuscripts
permalink: /publication/2025-09-28-mapping-overlaps-in-benchmarks
excerpt: "We connect benchmark semantics, performance, and token-level perplexity signatures to uncover hidden overlaps and define new evaluation axes for LLM benchmarking."
date: 2025-09-28
venue: "arXiv preprint"
paperurl: "https://arxiv.org/abs/2509.23488v1"
citation: 'Siyang Wu*, H. Bao*, S. Li*, Ari Holtzman, and James A. Evans. (2025). <i>Mapping Overlaps in Benchmarks through Perplexity in the Wild.</i> arXiv:2509.23488v1.'
---

We propose a **meta-evaluation framework** that uses *in-the-wild perplexity signatures* to map benchmark overlaps across 88 datasets and 32 models.  
By identifying **benchmark signatures**—sets of salient tokens predictive of benchmark performance—we reveal the latent functions benchmarks actually measure and provide a scalable method to assess benchmark validity and redundancy.
